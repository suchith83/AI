{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 87397,
          "databundleVersionId": 9945924,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "final_sub",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suchith83/AI/blob/main/final_sub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "qCLUGSeqEzHp"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjhLzudmE31Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "col_774_a_3_new_path = kagglehub.competition_download('col-774-a-3-new')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "oD3kTaS5EzHs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from torch import nn, optim\n",
        "# from torch.nn import functional as F\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tZ7cSDgwEzHs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data functions\n",
        "\n",
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "class CustomDatasetWrapper(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.subset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        ""
      ],
      "metadata": {
        "id": "zROIISJxEzHt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#SAM\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n"
      ],
      "metadata": {
        "id": "XjdbiOooEzHu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#WideResNet\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n",
        "\n",
        "# print('completed')"
      ],
      "metadata": {
        "id": "ghun1mAbEzHv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#bypassing batchnorm (disabling BN)\n",
        "\n",
        "def disable_running_stats(model):\n",
        "    def _disable(module):\n",
        "        if isinstance(module, _BatchNorm):\n",
        "            module.backup_momentum = module.momentum\n",
        "            module.momentum = 0\n",
        "\n",
        "    model.apply(_disable)\n",
        "\n",
        "def enable_running_stats(model):\n",
        "    def _enable(module):\n",
        "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
        "            module.momentum = module.backup_momentum\n",
        "\n",
        "    model.apply(_enable)\n"
      ],
      "metadata": {
        "id": "XGqqruY0EzHv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TemperatureScaling:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.temperature = torch.nn.Parameter(torch.ones(1) * 1.0)\n",
        "\n",
        "    def set_temperature(self, validation_loader):\n",
        "        self.model.eval()\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                logits = self.model(inputs)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(labels)\n",
        "\n",
        "        logits = torch.cat(logits_list)\n",
        "        labels = torch.cat(labels_list)\n",
        "\n",
        "        # Optimize temperature\n",
        "        def loss_fn():\n",
        "            return F.cross_entropy(logits / self.temperature, labels)\n",
        "\n",
        "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "        optimizer.step(lambda: optimizer.zero_grad() or loss_fn().backward())\n",
        "\n",
        "        return self.temperature.item()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        logits = self.model(inputs)\n",
        "        return logits / self.temperature"
      ],
      "metadata": {
        "id": "ZQ07FSX9EzHw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelwithT(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelwithT, self).__init__()\n",
        "        self.model = model\n",
        "        self.temp = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        logits = self.model(input)\n",
        "        return logits / self.temp\n",
        "\n",
        "    def set_temp(self, valid_loader):\n",
        "        self.cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                logits = self.model(input)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "\n",
        "            before_temp_nll = nll_criterion(logits, labels).item()\n",
        "            before_temp_ece = ece_criterion(logits, labels).item()\n",
        "\n",
        "            print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temp_nll, before_temp_ece))\n",
        "\n",
        "            optimizer = optim.LBFGS([self.temp], lr = 0.01, max_iter=50)\n",
        "\n",
        "            def eval():\n",
        "                optimizer.zero_grad()\n",
        "                loss = nll_criterion(self.temp_scale(logits), labels)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            optimizer.step(eval)\n",
        "\n",
        "            after_temp_nll = nll_criterion(self.temp_scale(logits), labels).item()\n",
        "            after_temp_ece = ece_criterion(self.temp_scale(logits), labels).item()\n",
        "\n",
        "            print(\"T: \", self.temp)\n",
        "            print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temp_nll, after_temp_ece))\n",
        "\n",
        "            return self\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0,1,n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            in_bin = confidences.gt(bin_lower.item())*confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "        return ece\n",
        "\n",
        "# print('completed')"
      ],
      "metadata": {
        "id": "aHRFLxdIEzHw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Cutout data augumentation\n",
        "# import torch\n",
        "\n",
        "\n",
        "class Cutout:\n",
        "    def __init__(self, size=16, p=0.5):\n",
        "        self.size = size\n",
        "        self.half_size = size // 2\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if torch.rand([1]).item() > self.p:\n",
        "            return image\n",
        "\n",
        "        left = torch.randint(-self.half_size, image.size(1) - self.half_size, [1]).item()\n",
        "        top = torch.randint(-self.half_size, image.size(2) - self.half_size, [1]).item()\n",
        "        right = min(image.size(1), left + self.size)\n",
        "        bottom = min(image.size(2), top + self.size)\n",
        "\n",
        "        image[:, max(0, left): right, max(0, top): bottom] = 0\n",
        "        return image\n",
        "\n",
        "\n",
        "#learning rate scheduler\n",
        "class StepLR:\n",
        "    def __init__(self, optimizer, learning_rate: float, total_epochs: int):\n",
        "        self.optimizer = optimizer\n",
        "        self.total_epochs = total_epochs\n",
        "        self.base = learning_rate\n",
        "\n",
        "    def __call__(self, epoch):\n",
        "        if epoch < self.total_epochs * 3/10:\n",
        "            lr = self.base\n",
        "        elif epoch < self.total_epochs * 6/10:\n",
        "            lr = self.base * 0.2\n",
        "        elif epoch < self.total_epochs * 8/10:\n",
        "            lr = self.base * 0.2 ** 2\n",
        "        else:\n",
        "            lr = self.base * 0.2 ** 3\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    def lr(self) -> float:\n",
        "        return self.optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "\n",
        "# print('completedd')"
      ],
      "metadata": {
        "id": "FsTkERBDEzHx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_classwise_accuracy(df, pred_col):\n",
        "\n",
        "    accuracy_dict = {}\n",
        "    grouped = df.groupby(pred_col)\n",
        "\n",
        "    for name, group in grouped:\n",
        "        accuracy = (group['Label'] == group[pred_col]).sum() / len(group)\n",
        "        accuracy_dict[name] = accuracy\n",
        "\n",
        "    return accuracy_dict\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, id_column_name: str, accuracy_threshold: float = 0.7, gamma: float = 5.0) -> float:\n",
        "    \"\"\"\n",
        "    Custom metric to evaluate model performance.\n",
        "\n",
        "    Returns the overall performance score based on correct classifications for high and low accuracy classes.\n",
        "\n",
        "    Parameters:\n",
        "        - solution: DataFrame containing the true class labels.\n",
        "        - submission: DataFrame containing the predicted class labels.\n",
        "        - id_column_name: Name of the column used for merging both DataFrames.\n",
        "        - accuracy_threshold: Threshold for class accuracy.\n",
        "        - gamma: Weighting factor for low accuracy classifications.\n",
        "\n",
        "    Returns:\n",
        "        - A single float representing the overall performance of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Merge solution and submission DataFrames using the ID column.\n",
        "    filtered_df = pd.merge(solution, submission, on=id_column_name)\n",
        "\n",
        "    # Assuming 'Label' comes from solution and 'Predicted_label' comes from submission\n",
        "    filtered_df = filtered_df[['Label', 'Predicted_label']]\n",
        "\n",
        "    # Exclude rows with -1 predictions\n",
        "    filtered_df = filtered_df[filtered_df['Predicted_label'] != -1]\n",
        "\n",
        "    all_classes = list(range(100))  # Assuming classes are from 0 to 99\n",
        "    sum_of_correctly_classified_high_accuracy = 0\n",
        "    sum_of_correctly_classified_low_accuracy = 0\n",
        "\n",
        "    # Calculate accuracy per class\n",
        "    accuracy_per_class = calculate_classwise_accuracy(filtered_df, 'Predicted_label')\n",
        "\n",
        "    for cls in all_classes:\n",
        "        total = len(filtered_df[filtered_df['Predicted_label'] == cls])\n",
        "        correct = (filtered_df[filtered_df['Predicted_label'] == cls]['Predicted_label'] == filtered_df[filtered_df['Predicted_label'] == cls]['Label']).sum()\n",
        "        class_accuracy = accuracy_per_class.get(cls, 0.0)\n",
        "\n",
        "        if class_accuracy >= accuracy_threshold:\n",
        "            sum_of_correctly_classified_high_accuracy += total\n",
        "        else:\n",
        "            sum_of_correctly_classified_low_accuracy += total\n",
        "\n",
        "    # Calculate final score\n",
        "    final_score = sum_of_correctly_classified_high_accuracy - gamma * sum_of_correctly_classified_low_accuracy\n",
        "\n",
        "    return float(final_score)\n",
        "\n",
        "# print('completed')"
      ],
      "metadata": {
        "id": "w_WrHsSIEzHx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#validate model\n",
        "def validate_model(model, val_loader, loss_function, device,confidences= 0.9):\n",
        "    model.eval()\n",
        "    # val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs =  torch.softmax(outputs, dim=1)\n",
        "            probility, pred_class = probs.max(dim=1)\n",
        "            # loss = loss_function(outputs, targets)\n",
        "            # val_loss += loss.item()\n",
        "            combined = zip(probility, pred_class)\n",
        "            for prob, pred in combined:\n",
        "                if prob > confidences:\n",
        "                    predictions.append(pred.item())\n",
        "                else:\n",
        "                    predictions.append(-1)\n",
        "            #add true labels\n",
        "            # Adding true labels for tracking\n",
        "            true_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Calculate the final score\n",
        "        solution = pd.DataFrame({'ID': range(len(true_labels)), 'Label': true_labels})\n",
        "        submission = pd.DataFrame({'ID': range(len(predictions)), 'Predicted_label': predictions})\n",
        "\n",
        "        score_value = score(solution, submission, id_column_name='ID')\n",
        "\n",
        "    # return\n",
        "\n",
        "    return 100. * correct / total,score_value\n",
        "\n",
        "\n",
        "# print('completedd')"
      ],
      "metadata": {
        "id": "vBTv2pmqEzHy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_model(model, valid_loader,device):\n",
        "    model.eval()\n",
        "    scaled_model = ModelwithT(model)\n",
        "    scaled_model = scaled_model.to(device)\n",
        "    scaled_model.set_temp(valid_loader)\n",
        "\n",
        "    return scaled_model\n",
        "# print('completed')"
      ],
      "metadata": {
        "id": "eMlnRHb-EzHy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "\n",
        "def train_model(model, train_loader,validation_loader, optimizer, loss_function,scheduler, epochs,device):\n",
        "\n",
        "    best_model = None\n",
        "    best_score = float('-inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        # correct = 0\n",
        "        # total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            #first forward-backward step\n",
        "            enable_running_stats(model)\n",
        "            predictions = model(inputs)\n",
        "            loss = loss_function(predictions, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # second forward-backward step\n",
        "            disable_running_stats(model)\n",
        "            loss_function(model(inputs), targets).backward()\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler(epoch)\n",
        "\n",
        "        temp_scaled_model = scaled_model(model, train_loader,device)\n",
        "\n",
        "\n",
        "        #validation\n",
        "\n",
        "        val_accuracy, val_score = validate_model(temp_scaled_model, validation_loader, loss_function, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Val Acc: {val_accuracy:.2f}%, Val Score: {val_score}')\n",
        "\n",
        "        if val_score > best_score:\n",
        "            best_score = val_score\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "\n",
        "    return best_model, best_score\n",
        "\n",
        "# print('completed')"
      ],
      "metadata": {
        "id": "kHZtcqelEzHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#create model\n",
        "\n",
        "depth = 28\n",
        "widen_factor = 10\n",
        "num_classes = 100\n",
        "drop_rate = 0.3\n",
        "# labelsmoothing = 0.1\n",
        "epochs = 60\n",
        "weightdecay = 5e-4\n",
        "momentum_factor = 0.9\n",
        "batchsize = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = WideResNet(depth, num_classes, widen_factor, drop_rate)\n",
        "model = model.to(device)\n",
        "\n",
        "# loss_function = nn.CrossEntropyLoss(label_smoothing=labelsmoothing)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "base_optimizer = torch.optim.SGD\n",
        "isnesterov = True\n",
        "rho = 0.05\n",
        "\n",
        "optimizer = SAM(model.parameters(), base_optimizer= base_optimizer, lr = 0.1, weight_decay = weightdecay, momentum = momentum_factor,dampening=0,nesterov=isnesterov)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer, T_max=epochs)\n",
        "\n",
        "scheduler = StepLR(optimizer, learning_rate=0.1, total_epochs=epochs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        Cutout(size=16,p = 0.5),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    normalize\n",
        "])\n",
        "\n",
        "\n",
        "train_data = CIFAR100Dataset('/kaggle/input/col-774-a-3/train.pkl', transform=None)\n",
        "# train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=2, pin_memory=True)\n",
        "# test_data = CIFAR100Dataset('/kaggle/input/col-774-a-3/test.pkl', transform=transform_test)\n",
        "\n",
        "\n",
        "\n",
        "# Set the size of the training and validation sets\n",
        "train_size = 30000\n",
        "val_size = 10000\n",
        "\n",
        "# Split the dataset once into training and validation subsets\n",
        "train_subset, val_subset = random_split(train_data, [train_size, val_size])\n",
        "\n",
        "train_subset = CustomDatasetWrapper(train_subset, transform=transform_train)\n",
        "val_subset = CustomDatasetWrapper(val_subset, transform=transform_test)\n",
        "\n",
        "\n",
        "# DataLoader for each subset\n",
        "train_loader = DataLoader(train_subset, batch_size=batchsize, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=batchsize, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# Measure the start time\n",
        "start_time = time.time()\n",
        "print('Training')\n",
        "# Train the model\n",
        "best_model, best_score = train_model(model, train_loader, val_loader, optimizer, loss_function, scheduler, epochs, device)\n",
        "\n",
        "\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# save model\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "# Measure the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the total time taken\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"Training completed in {total_time:.2f} seconds\")\n",
        "print('completed')"
      ],
      "metadata": {
        "id": "czRLcBYFEzHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        Cutout(size=16,p = 0.5),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    normalize\n",
        "])\n",
        "\n",
        "batchsize = 128\n",
        "\n",
        "train_data = CIFAR100Dataset('/kaggle/input/col-774-a-3/train.pkl', transform=transform_train)\n",
        "test_data = CIFAR100Dataset('/kaggle/input/col-774-a-3/test.pkl', transform=transform_test)\n",
        "\n",
        "# Data loader for train, test\n",
        "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "BtMDMwd1EzHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#create model\n",
        "\n",
        "depth = 28\n",
        "widen_factor = 10\n",
        "num_classes = 100\n",
        "drop_rate = 0.3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "fin_model = WideResNet(depth, num_classes, widen_factor, drop_rate)\n",
        "fin_model = fin_model.to(device)\n",
        "\n",
        "# fin_model.load_state_dict(torch.load('/kaggle/input/v2/pytorch/default/1/model_hari.pth'))\n",
        "fin_model.load_state_dict(torch.load('/kaggle/input/v1/pytorch/default/1/model.pth'))"
      ],
      "metadata": {
        "id": "FSxD5O9eEzHz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fin_model.eval()\n",
        "scal_model = scaled_model(fin_model, train_loader, device)\n",
        "scal_model.eval()\n",
        "torch.save(scal_model, 'scaled_model2.pth')\n",
        "\n",
        "valid_loader = DataLoader(train_data, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "8mgIINPEEzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for X_val, Y_val in valid_loader:\n",
        "#         if count % 100 == 0:\n",
        "#             print(len(predictions), count)\n",
        "        X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
        "        outputs = scal_model(X_val)\n",
        "\n",
        "        predicted = torch.softmax(outputs,dim=1)\n",
        "\n",
        "        predicted_class = predicted.argmax(dim=1).item()\n",
        "        predicted_prob = predicted[0,predicted_class].item()\n",
        "\n",
        "        predictions.append((predicted_prob,predicted_class,Y_val.item()))\n",
        "\n",
        "#         print((predicted_prob,predicted_class,Y_val.item()))\n",
        "\n",
        "        if predicted_class == Y_val.item():\n",
        "            count += 1\n",
        "\n",
        "print(\"accuracy: \", count/40000)"
      ],
      "metadata": {
        "id": "4ADu1UrSEzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "h = {(1)*i:[0,0] for i in range(1,11)}\n",
        "for prob,pc,oc in predictions:\n",
        "    l,r = 0,1\n",
        "    while r <= 10 :\n",
        "        if prob >= l/10 and prob < r/10:\n",
        "            h[r][1] += 1\n",
        "            if pc == oc:\n",
        "                h[r][0] += 1\n",
        "            break\n",
        "        else:\n",
        "            l = r\n",
        "            r += 1\n",
        "print(h)"
      ],
      "metadata": {
        "id": "_3QgETOtEzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tau = 0.9925\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for X,id in test_loader:\n",
        "         #dimesnsions = batchsize x channels x height x width = batchsize x 1 x 50 x 100\n",
        "        X = X.to(device)\n",
        "#         print(X.shape)\n",
        "        outputs = scal_model(X)    #dimesnsions = batchsize x k = batchsize x 8\n",
        "        # print('outputs' , outputs)\n",
        "\n",
        "\n",
        "        predicted =  torch.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
        "        # print(\"predicted: \", predicted)\n",
        "\n",
        "\n",
        "        predicted_class = predicted.argmax(dim=1).item()\n",
        "        # print(\"class: \", predicted[0, predicted_class].item())\n",
        "        if predicted[0, predicted_class].item() > tau:\n",
        "            # Keep the predicted class\n",
        "            final_predicted_class = predicted_class\n",
        "        else:\n",
        "            # If the probability is less than the threshold, assign -1\n",
        "            final_predicted_class = -1\n",
        "\n",
        "        test_predictions.append([id.item(),final_predicted_class])\n",
        "#chnage the predictions to numpy array\n",
        "test_predictions = np.array(test_predictions)"
      ],
      "metadata": {
        "id": "OTEdwHyTEzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = test_predictions[:, 1]\n",
        "unique_values, counts = np.unique(tmp, return_counts=True)\n",
        "unique_values, counts"
      ],
      "metadata": {
        "id": "-qdZO0-JEzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "output_file = 'submission3_3.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow(['ID', 'Predicted_label'])\n",
        "\n",
        "    # Write the data\n",
        "    writer.writerows(test_predictions)\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "YM1T_Mq2EzH0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class PlattScaling:\n",
        "    def __init__(self, model):\n",
        "        self.model = model.cuda()  # Move the model to GPU if available\n",
        "        self.platt_models = []\n",
        "\n",
        "    def fit(self, validation_loader):\n",
        "        logits_list, labels_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs = inputs.cuda()  # Move inputs to GPU\n",
        "                logits = self.model(inputs)\n",
        "                logits_list.extend(logits.cpu().numpy())  # Move logits back to CPU for sklearn\n",
        "                labels_list.extend(labels.cpu().numpy())  # Move labels back to CPU\n",
        "\n",
        "        logits = np.array(logits_list)\n",
        "        labels = np.array(labels_list)\n",
        "\n",
        "        # Fit one logistic regression per class (One-vs-Rest approach for multiclass)\n",
        "        for class_idx in range(logits.shape[1]):\n",
        "            clf = LogisticRegression()\n",
        "            clf.fit(logits[:, [class_idx]], labels == class_idx)\n",
        "            self.platt_models.append(clf)\n",
        "\n",
        "    def predict_proba(self, inputs):\n",
        "        inputs = inputs.cuda()  # Move inputs to GPU\n",
        "        logits = self.model(inputs).cpu().detach().numpy()  # Get logits, move to CPU\n",
        "        calibrated_probs = np.zeros_like(logits)\n",
        "\n",
        "        for class_idx, clf in enumerate(self.platt_models):\n",
        "            calibrated_probs[:, class_idx] = clf.predict_proba(logits[:, [class_idx]])[:, 1]\n",
        "\n",
        "        return calibrated_probs\n",
        ""
      ],
      "metadata": {
        "id": "epLyJyZTEzH0"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}